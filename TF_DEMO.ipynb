{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import rnn\n",
    "from reader import TokReader\n",
    "import pickle\n",
    "with open('tok_map.pkl', 'rb') as f:\n",
    "    tok_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel():\n",
    "    def __init__(self, config):\n",
    "        sent_len = config.sent_len\n",
    "        batch_size = config.batch_size\n",
    "        vocab_size = config.vocab_size\n",
    "        embed_size = config.embed_size\n",
    "        num_layers = config.num_layers\n",
    "        state_size = config.state_size\n",
    "        keep_prob = config.keep_prob\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, sent_len])\n",
    "        self.lengths = tf.placeholder(tf.int64, [batch_size])\n",
    "        self.targets = tf.placeholder(tf.float32, [batch_size, 1])\n",
    "\n",
    "        # Get embedding layer which requires CPU\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embeding = tf.get_variable(\"embedding\", [vocab_size, embed_size])\n",
    "            inputs = tf.nn.embedding_lookup(embeding, self.input_data)\n",
    "\n",
    "        #LSTM 1 -> Encode the characters of every tok into a fixed dense representation\n",
    "        with tf.variable_scope(\"rnn1\", reuse=None):\n",
    "            cell = rnn_cell.LSTMCell(state_size, input_size=embed_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            back_cell = rnn_cell.LSTMCell(state_size, input_size=embed_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            cell = rnn_cell.DropoutWrapper(\n",
    "              cell, input_keep_prob=keep_prob,\n",
    "                         output_keep_prob=keep_prob)\n",
    "            back_cell = rnn_cell.DropoutWrapper(\n",
    "              back_cell, input_keep_prob=keep_prob,\n",
    "                              output_keep_prob=keep_prob) \n",
    "            cell = rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "            backcell = rnn_cell.MultiRNNCell([back_cell] * num_layers)\n",
    "            \n",
    "            rnn_splits = [tf.squeeze(input_, [1]) for input_ in tf.split(1, sent_len, inputs)]\n",
    "            \n",
    "            self.shape1 = tf.shape(rnn_splits[0])\n",
    "\n",
    "            # Run the bidirectional rnn\n",
    "            outputs, last_fw_state, last_bw_state = rnn.bidirectional_rnn(\n",
    "                                                        cell, backcell, rnn_splits,\n",
    "                                                        sequence_length=self.lengths,\n",
    "                                                        dtype=tf.float32)\n",
    "        self.check1 = outputs[0]\n",
    "        self.shape2 = tf.shape(outputs[0])\n",
    "        sent_out = tf.concat(1, [last_fw_state, last_bw_state])\n",
    "        self.shape3 = tf.shape(sent_out)\n",
    "        #sent_out = outputs[-1]\n",
    "        #sent_out = tf.add_n(outputs)\n",
    "        output_size = state_size*4\n",
    "\n",
    "        with tf.variable_scope(\"linear\", reuse=None):\n",
    "            w = tf.get_variable(\"w\", [output_size, 1])\n",
    "            b = tf.get_variable(\"b\", [1], initializer=tf.constant_initializer(0.0))\n",
    "            raw_logits = tf.matmul(sent_out, w) + b \n",
    "        self.probabilities = tf.sigmoid(raw_logits)\n",
    "        self.cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(raw_logits, self.targets))\n",
    "\n",
    "        #Calculate gradients and propagate\n",
    "        #Aggregation method 2 is really important for rnn per the tensorflow issues list\n",
    "        tvars = tf.trainable_variables()\n",
    "        self.lr = tf.Variable(0.0, trainable=False) #Assign to overwrite\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        grads, _vars = zip(*optimizer.compute_gradients(self.cost, tvars, aggregation_method=2))\n",
    "        grads, self.grad_norm = tf.clip_by_global_norm(grads,\n",
    "                                      config.max_grad_norm)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, _vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    init_scale = 0.05\n",
    "    learning_rate = 0.1\n",
    "    max_grad_norm = 5\n",
    "    batch_size = 32\n",
    "    sent_len = 10\n",
    "    num_layers = 1\n",
    "    keep_prob = 0.5\n",
    "    vocab_size = len(tok_map) \n",
    "    state_size = 256\n",
    "    embed_size = 256\n",
    "    num_models = 100\n",
    "    num_epochs = 100\n",
    "    save_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:USF.reader:Instantiating TokReader object: training\n",
      "INFO:USF.reader:Loading reviews\n",
      "INFO:USF.reader:Instantiating TokReader object: valid\n",
      "INFO:USF.reader:Loading reviews\n"
     ]
    }
   ],
   "source": [
    "stream = TokReader(Config.sent_len, Config.batch_size, tok_map, random=True, \n",
    "                           rounded=True, training=True, limit=1000)\n",
    "validstream = TokReader(Config.sent_len, Config.batch_size, tok_map, random=True, \n",
    "                                rounded=True, training=False, limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_one(model, session, reader, training):\n",
    "    x,y,lengths = next(reader)\n",
    "    num_data_points = len(x)\n",
    "    feed_dict = {model.input_data:x, model.targets:y,\n",
    "                 model.lengths:lengths}\n",
    "    if training:\n",
    "        fetches =  [model.cost, model.grad_norm, model.shape1,\n",
    "                    model.shape2, model.shape3, model.check1, \n",
    "                    model.train_op]\n",
    "        cost, grad_norm, s1, s2, s3, c1, _  = session.run(fetches, feed_dict)\n",
    "        print(\"Check1 \", c1[0,:50])\n",
    "        print(\"Shape1 \",s1)\n",
    "        print(\"Shape2 \",s2)\n",
    "        print(\"Shape3 \",s3)\n",
    "        print(\"Cost: \", cost)\n",
    "        print(\"Grad norm: \", grad_norm)\n",
    "        \n",
    "    else:\n",
    "        print(\"Test step: \",step)\n",
    "        fetches =  self.probabilities\n",
    "        proba = session.run(fetches, feed_dict) \n",
    "        choice = np.where(proba > 0.5, 1, 0)\n",
    "        accuracy = np.mean(choice == y)\n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initializer = tf.random_uniform_initializer(-Config.init_scale,\n",
    "                                             Config.init_scale)\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope(\"model\", reuse=False, initializer=initializer):    \n",
    "    m = RNNModel(Config)\n",
    "    tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:USF.reader:Shuffling input data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check1  [ 0.00631031  0.         -0.         -0.         -0.00852726  0.00900993\n",
      "  0.00618966 -0.01362468  0.0075122  -0.01496265 -0.0045309   0.00410457\n",
      "  0.00936645  0.          0.          0.00855165 -0.         -0.00106325\n",
      " -0.00031307 -0.01047822  0.00430402 -0.         -0.01205309 -0.\n",
      "  0.00036303 -0.00471735 -0.         -0.01158424 -0.          0.\n",
      "  0.00927435  0.          0.00185948  0.01082929  0.         -0.          0.\n",
      " -0.         -0.          0.         -0.01465058  0.          0.00034434\n",
      " -0.01125397 -0.00193667 -0.01474594 -0.         -0.         -0.01042028\n",
      " -0.        ]\n",
      "Shape1  [ 32 256]\n",
      "Shape2  [ 32 512]\n",
      "Shape3  [  32 1024]\n",
      "Cost:  0.694444\n",
      "Grad norm:  0.863772\n"
     ]
    }
   ],
   "source": [
    "run_one(m, sess, stream.get_sents(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
